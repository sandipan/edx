# -*- coding: utf-8 -*-
"""
Created on Wed Feb 08 16:39:08 2017

@author: Sandipan.Dey
"""

import sys
import numpy as np
from numpy import transpose, matmul, exp, dot
from numpy.linalg import inv, det
import matplotlib.mlab as mlab
from matplotlib.pylab import plt
from random import randint
from mpl_toolkits.mplot3d import axes3d

# from sklearn.linear_model import Ridge

# python hw3_clustering.py X.csv

def gen_data(X_file, K):
    n_k = 100
    d = 2
    X = np.random.normal(0, 1, n_k)
    for i in range(1, d):
        X = transpose(np.array([X, np.random.normal(0, 1, n_k)]))
    for k in range(1, K):
        mu_k = randint(3, 8)
        sigma_k = randint(1, 2)
        X1 = np.random.normal(mu_k, sigma_k, n_k)
        for i in range(1, d):
            mu_k = randint(3, 8) #10)
            sigma_k = randint(1, 2) #8)
            X1 = transpose(np.array([X1, np.random.normal(mu_k, sigma_k, n_k)]))
        X = np.concatenate([X, X1], 0)
    np.savetxt(X_file, X, delimiter=",")
    
def kmeans(X, K, niter=10):
    
    n = X.shape[0]
    
    # initialize
    mu = X[np.random.choice(range(n), K),:]
    c = np.zeros(K)
    
    for iter in range(1,niter+1): 
      # coordinate descent  
      # optimize w.r.t. cluster assignment c_i = k  
      print 'iteration', iter
      f = open("centroids-"+str(iter)+".csv","w")
      c = [min([(sum((X[i,:]-mu[k])**2), k) for k in range(K)])[1] for i in range(n)]  
      # optimize w.r.t. centorid update mu_k
      mu = np.array([np.mean([X[i,:] for i in range(n) if c[i] == k], axis=0) for k in range(K)])
      for k in range(K):
          #print mu[k, :]
          f.write(','.join(map(str, mu[k, :])) + '\n')
      f.close()
      # plot
      plt.scatter(X[:,0], X[:,1], c=map(lambda x:'r' if x == 1 else ('g' if x == 2 else 'b'), c))
      plt.scatter(mu[:,0], mu[:,1], c='k', s=100, marker='*')
      plt.title('Kmeans Iteration ' + str(iter))
      plt.savefig('kmeans/fig' + str(iter).zfill(2))
      plt.close()

def weighted_kmeans(X, K, niter=10):
    
    n = X.shape[0]
    
    # initialize
    mu = X[np.random.choice(range(n), K),:]
    phi = np.reshape(np.zeros(n*K), (n,K))
    beta = 1 #10
    
    for iter in range(1,niter+1): 
      # coordinate descent  
      # optimize w.r.t. cluster assignment c_i = k  
      print 'iteration', iter
      #f = open("centroids-"+str(iter)+".csv","w")
      for i in range(n):
          for k in range(K):
              phi[i,k] = exp(-sum((X[i,:]-mu[k,:])**2)/beta)
          #print sum(phi[i,:])        
          phi[i,:] = phi[i,:] / sum(phi[i,:])    
      # optimize w.r.t. centorid update mu_k
      mu = np.array([np.sum([phi[i,k]*X[i,:] for i in range(n)], axis=0) / sum(phi[:,k]) for k in range(K)])
      #for k in range(K):
          #print mu[k, :]
          #f.write(','.join(map(str, mu[k, :])) + '\n')
      #f.close()
      # plot
      #plt.scatter(X[:,0], X[:,1], c=np.argmax(phi, axis=1))
      plt.scatter(X[:,0], X[:,1], c=phi[:,:3])
      #plt.scatter(X[:,0], X[:,1], c=np.argsort(phi, axis=1)[:,-3:])
      
      plt.scatter(mu[:,0], mu[:,1], c='k', s=100, marker='*')
      plt.title('Weighted Kmeans Iteration ' + str(iter))
      plt.savefig('wkmeans/fig' + str(iter).zfill(2))
      plt.close()

# convert -delay 30 -loop 0 *.png kmeans.gif
      
def GMM_EM(X, K, niter=10):
    
    n, d = X.shape
    
    # initialize
    n_k = np.zeros(K)
    phi = np.reshape(np.zeros(n*K), (n,K))
    #mu = np.reshape(np.random.normal(0,1,K*d), (K,d))
    mu = X[np.random.choice(range(n), K),:]
    sigma = np.reshape(np.zeros(K*d*d), (K,d,d))
    for k in range(K):
        sigma[k,:,:] = np.eye(d)
    pi = np.ones(K) / (K*1.)
    
    Ls = []
    for iter in range(1,niter+1): 
      # EM
      # E-Step
      print 'iteration', iter
      f1 = open("pi-"+str(iter)+".csv","w")
      f2 = open("mu-"+str(iter)+".csv","w")
      L = sum([np.log(sum([pi[k]*(1./(2*np.pi*det(sigma[k,:,:])))*exp(-matmul(matmul(transpose(X[i,:]-mu[k,:]), inv(sigma[k,:,:])), X[i,:]-mu[k,:])/2) for k in range(K)])) for i in range(n)])
      Ls.append(L)
      print iter, L
      for i in range(n):
          for k in range(K):
              # prob in cluster k and prob generated by the gaussain
              #phi[i,k] = pi[k]* det(sigma[k,:,:])**(-0.5) * np.exp(-matmul(matmul(transpose(X[i,:]-mu[k,:]), inv(sigma[k,:,:])), X[i,:]-mu[k,:])/2)
              phi[i,k] = exp(np.log(pi[k]) - np.log(det(sigma[k,:,:]))/2 - matmul(matmul(transpose(X[i,:]-mu[k,:]), inv(sigma[k,:,:])), X[i,:]-mu[k,:])/2)
          #print sum(phi[i,:])        
          phi[i,:] = phi[i,:] / sum(phi[i,:])    
      #print phi
      # M-Step
      for k in range(K):
          n_k[k] = sum(phi[:,k])
          #print mu[k, :]
          pi[k] = n_k[k] / n
          mu[k,:] = np.sum([phi[i,k]*X[i,:] for i in range(n)], axis=0) / n_k[k]
          sigma[k,:,:] = sum([phi[i,k]*np.outer(X[i,:]-mu[k,:], X[i,:]-mu[k,:]) for i in range(n)]) / n_k[k]
          #mu[k,:], sigma[k,:,:] = np.zeros(d), np.reshape(np.zeros(d*d), (d,d))
          #for i in range(n): mu[k,:] += phi[i,k]*X[i,:]
          #mu[k,:] /= n_k[k]    
          #for i in range(n): sigma[k,:,:] += phi[i,k]*np.outer(X[i,:]-mu[k,:], X[i,:]-mu[k,:])
          #sigma[k,:,:] /= n_k[k]
          
          f1.write(str(pi[k]) + '\n')
          f2.write(','.join(map(str, mu[k,:])) + '\n')
          np.savetxt("Sigma-"+str(k+1)+"-"+str(iter)+".csv", sigma[k,:,:], delimiter=",")
      
      #print mu
      #print sigma
      f1.close()
      f2.close()
      # plot
      #plt.scatter(X[:,0], X[:,1], c=np.argmax(phi, axis=1))
      #fig = plt.figure()
      #ax = fig.add_subplot(111, projection="3d")
      from matplotlib.ticker import LinearLocator, FormatStrFormatter
      from mpl_toolkits.mplot3d import axes3d
      from matplotlib import cm
      from matplotlib.colors import LightSource
      light = LightSource(90, 45)
 
      col = {0:'r', 1:'g', 2:'b'}
      fig = plt.figure()
      ax = fig.gca(projection='3d')
      ax.scatter(X[:,0], X[:,1], c=phi[:,:3])
      delta = 0.025
      x1 = np.arange(min(X[:,0]), max(X[:,0]), delta)
      x2 = np.arange(min(X[:,1]), max(X[:,1]), delta)
      X1, X2 = np.meshgrid(x1, x2)
      Z1 = None    
      for k in range(K):        
          Z = mlab.bivariate_normal(X1, X2, mux=mu[k,0], muy=mu[k,1], sigmax=sigma[k,0,0], sigmay=sigma[k,1,1], sigmaxy=sigma[k,0,1])
          Z1 = Z if Z1 == None else 3*Z + Z1
          #ax.plot_surface(X1, X2, Z, cmap="autumn_r", lw=0.5, rstride=1, cstride=1, alpha=0.5)
          #CS = ax.contour(X1, X2, Z, c=k) #, levels=np.arange(-1.2, 1.6, 0.2))
          CS = ax.contour(X1, X2, Z, 3, zdir='z', offset=-0.4, colors=col[k]) #, levels=np.arange(-1.2, 1.6, 0.2))
          plt.clabel(CS, fontsize=9, inline=1)
          
      illuminated_surface = light.shade(Z1, cmap=cm.Spectral)#cm.coolwarm)
      ax.plot_surface(X1, X2, Z1, cmap=plt.cm.afmhot_r,
                    facecolors=illuminated_surface, lw=2, alpha=0.5)#, zorder = 0.1*i+0.1) #, rstride=8, cstride=8, alpha=0.2, color=col[i], linewidth=0, antialiased=True)
        
      ax.set_xlabel('X1')
      #ax.set_xlim(-5, 8)
      ax.set_ylabel('X2')
      #ax.set_ylim(-40, 40)
      ax.set_zlabel('Y')
      ax.zaxis.set_major_locator(LinearLocator(10))
      ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))
      ax.set_zlim(-0.4, 0.4)
  
      plt.title('GMM-EMM Iteration ' + str(iter))
      plt.savefig('gmmem/fig' + str(iter).zfill(2))
      plt.close()
      
    plt.plot(list(range(niter)), Ls, 'b-')
    plt.xlabel('# Iterations')
    plt.ylabel('log likelihood')
    plt.show()

if len(sys.argv) >= 2:

    X_file = sys.argv[1] 
    K = 3 #3 #5 #10
    gen_data(X_file, K)    
    X = np.genfromtxt(X_file,delimiter=',')
    #kmeans(X, K)
    #weighted_kmeans(X, K)
    GMM_EM(X, K)

else:

    None
    
